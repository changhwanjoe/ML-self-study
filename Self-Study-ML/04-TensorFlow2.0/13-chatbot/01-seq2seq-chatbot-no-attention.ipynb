{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq 모델을 활용한 챗봇 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq 모델의 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋에 필요한 라이브러리를 다운로드 받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Korpora`는 한글 자연어처리 데이터)셋입니다.\n",
    "\n",
    "- [깃헙 주소 링크](https://github.com/ko-nlp/Korpora)\n",
    "- [공식 도큐먼트](https://pypi.org/project/Korpora/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설치 명령어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 중 챗봇용 데이터셋인 `KoreanChatbotKorpus`를 다운로드 받습니다.\n",
    "- `KoreanChatbotKorpus` 데이터셋을 활용하여 챗봇 모델을 학습합니다.\n",
    "- text, pair로 구성되어 있습니다.\n",
    "- 질의는 **text**, 답변은 **pair**입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import KoreanChatbotKorpus\n",
    "corpus = KoreanChatbotKorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예시 텍스트를 보면 구어체로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_all_texts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_all_pairs()`는 `text`와 `pair`가 쌍으로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_all_pairs()[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_all_pairs()[0].pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question**과 **answer**를 분리합니다.\n",
    "\n",
    "**question**은 질의로 활용될 데이터셋, **answer**는 답변으로 활용될 데이터 셋입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "pairs = []\n",
    "\n",
    "for sentence in corpus.get_all_pairs():\n",
    "    texts.append(sentence.text)\n",
    "    pairs.append(sentence.pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(texts, pairs))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특수문자는 제거합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**한글과 숫자를 제외한 특수문자를 제거**하도록 합니다.\n",
    "\n",
    "*[참고] 튜토리얼에서는 특수문자와 영문자를 제거하나, 실제 프로젝트에 적용해보기 위해서는 신중히 결정해야합니다.*\n",
    "\n",
    "*챗봇 대화에서 영어도 많이 사용되고, 특수문자도 굉장히 많이 사용됩니다. 따라서, 선택적으로 제거할 특수기호나 영문자를 정의한 후에 전처리를 진행하야합니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re 모듈은 regex expression을 적용하기 위하여 활용합니다.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n",
    "    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**적용한 예시**\n",
    "\n",
    "한글, 숫자 이외의 모든 문자를 전부 제거됨을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence('12시 땡^^!??')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence('abcef가나다^^$%@12시 땡^^!??')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 형태소 분석기 (Konlpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석기를 활용하여 문장을 분리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```가방에 들어가신다 -> 가방/NNG + 에/JKM + 들어가/VV + 시/EPH + ㄴ다/EFN```\n",
    "\n",
    "- **형태소 분석** 이란 형태소를 비롯하여, 어근, 접두사/접미사, 품사(POS, part-of-speech) 등 다양한 언어적 속성의 구조를 파악하는 것입니다.\n",
    "- **konlpy 형태소 분석기를 활용**하여 한글 문장에 대한 토큰화처리를 보다 효율적으로 처리합니다.\n",
    "\n",
    "\n",
    "\n",
    "[공식 도큐먼트](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**설치**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "konlpy 내부에는 Kkma, Okt, Twitter 등등의 형태소 분석기가 존재하지만, 이번 튜토리얼에서는 Okt를 활용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 변환에 활용하는 함수\n",
    "# morphs 함수 안에 변환한 한글 문장을 입력 합니다.\n",
    "def process_morph(sentence):\n",
    "    return ' '.join(okt.morphs(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seq2Seq** 모델이 학습하기 위한 데이터셋을 구성할 때, 다음과 같이 **3가지 데이터셋**을 구성합니다.\n",
    "\n",
    "- `question`: encoder input 데이터셋 (질의 전체)\n",
    "- `answer_input`: decoder input 데이터셋 (답변의 시작). START 토큰을 문장 처음에 추가 합니다.\n",
    "- `answer_output`: decoder output 데이터셋 (답변의 끝). END 토큰을 문장 마지막에 추가 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_morph(sentence, is_question=True):\n",
    "    # 한글 문장 전처리\n",
    "    sentence = clean_sentence(sentence)\n",
    "    # 형태소 변환\n",
    "    sentence = process_morph(sentence)\n",
    "    # Question 인 경우, Answer인 경우를 분기하여 처리합니다.\n",
    "    if is_question:\n",
    "        return sentence\n",
    "    else:\n",
    "        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n",
    "        return ('<START> ' + sentence, sentence + ' <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, pairs):\n",
    "    questions = []\n",
    "    answer_in = []\n",
    "    answer_out = []\n",
    "\n",
    "    # 질의에 대한 전처리\n",
    "    for text in texts:\n",
    "        # 전처리와 morph 수행\n",
    "        question = clean_and_morph(text, is_question=True)\n",
    "        questions.append(question)\n",
    "\n",
    "    # 답변에 대한 전처리\n",
    "    for pair in pairs:\n",
    "        # 전처리와 morph 수행\n",
    "        in_, out_ = clean_and_morph(pair, is_question=False)\n",
    "        answer_in.append(in_)\n",
    "        answer_out.append(out_)\n",
    "    \n",
    "    return questions, answer_in, answer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answer_in, answer_out = preprocess(texts, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_in[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_out[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = questions + answer_in + answer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (' '.join(questions) + ' '.join(answer_in) + ' '.join(answer_out)).split()\n",
    "len(set(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# WARNING 무시\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**토큰의 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer**로 문장에 대한 Word-Index Vocabulary(단어 사전)을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단어 사전 10개 출력**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in tokenizer.word_index.items():\n",
    "    print(f'{word}\\t\\t => \\t{idx}')\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**토큰의 갯수 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 치환: 텍스트를 시퀀스로 인코딩 (`texts_to_sequences`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_sequence = tokenizer.texts_to_sequences(questions)\n",
    "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
    "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장의 길이 맞추기 (`pad_sequences`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "answer_in_padded = pad_sequences(answer_in_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "answer_out_padded = pad_sequences(answer_out_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_in_padded.shape, answer_out_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습용 인코더 (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.lstm = LSTM(units, return_state=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dropout(x)\n",
    "        x, hidden_state, cell_state = self.lstm(x)\n",
    "        return [hidden_state, cell_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습용 디코더 (Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.lstm = LSTM(units, \n",
    "                         return_state=True, \n",
    "                         return_sequences=True, \n",
    "                        )\n",
    "        self.dense = Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, initial_state):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dropout(x)\n",
    "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)        \n",
    "        x = self.dense(x)\n",
    "        return x, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
    "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        if training:\n",
    "            encoder_inputs, decoder_inputs = inputs\n",
    "            context_vector = self.encoder(encoder_inputs)\n",
    "            decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs, initial_state=context_vector)\n",
    "            return decoder_outputs\n",
    "        else:\n",
    "            context_vector = self.encoder(inputs)\n",
    "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
    "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
    "            \n",
    "            for i in tf.range(self.time_steps):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq, initial_state=context_vector)\n",
    "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
    "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
    "                results = results.write(i, decoder_output)\n",
    "                \n",
    "                if decoder_output == self.end_token:\n",
    "                    break\n",
    "                    \n",
    "                target_seq = decoder_output\n",
    "                context_vector = [decoder_hidden, decoder_cell]\n",
    "                \n",
    "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어별 원핫인코딩 적용\n",
    "\n",
    "단어별 원핫인코딩을 적용하는 이유는 decoder의 output(출력)을 원핫인코딩 vector로 변환하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(padded):\n",
    "    # 원핫인코딩 초기화\n",
    "    one_hot_vector = np.zeros((len(answer_out_padded), MAX_LENGTH, VOCAB_SIZE))\n",
    "\n",
    "    # 디코더 목표를 원핫인코딩으로 변환\n",
    "    # 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
    "    for i, sequence in enumerate(answer_out_padded):\n",
    "        for j, index in enumerate(sequence):\n",
    "            one_hot_vector[i, j, index] = 1\n",
    "\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
    "answer_out_one_hot = convert_to_one_hot(answer_out_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 12638), (30, 12638))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_in_one_hot[0].shape, answer_in_one_hot[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변환된 index를 다시 단어로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_index_to_text(indexs, end_token): \n",
    "    \n",
    "    sentence = ''\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == end_token:\n",
    "            # 끝 단어이므로 예측 중비\n",
    "            break;\n",
    "        # 사전에 존재하는 단어의 경우 단어 추가\n",
    "        if index > 0 and tokenizer.index_word[index] is not None:\n",
    "            sentence += tokenizer.index_word[index]\n",
    "        else:\n",
    "        # 사전에 없는 인덱스면 빈 문자열 추가\n",
    "            sentence += ''\n",
    "            \n",
    "        # 빈칸 추가\n",
    "        sentence += ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**하이퍼 파라미터 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 100\n",
    "TIME_STEPS = MAX_LENGTH\n",
    "START_TOKEN = tokenizer.word_index['<START>']\n",
    "END_TOKEN = tokenizer.word_index['<END>']\n",
    "\n",
    "UNITS = 128\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "DATA_LENGTH = len(questions)\n",
    "SAMPLE_SIZE = 3\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**체크포인트 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'model/seq2seq-chatbot-no-attention-checkpoint.ckpt'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True, \n",
    "                             monitor='loss', \n",
    "                             verbose=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**분산환경 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "분산환경 사용 >> GPU: 2\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "FLAG = True\n",
    "if strategy.num_replicas_in_sync  > 1 and FLAG:\n",
    "    MULTIPLE_BATCH = strategy.num_replicas_in_sync\n",
    "    print(f'분산환경 사용 >> GPU: {MULTIPLE_BATCH}')\n",
    "else:\n",
    "    print(f'분산환경 미사용')\n",
    "    MULTIPLE_BATCH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 생성 & compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분산환경 사용 >> GPU: 2\n"
     ]
    }
   ],
   "source": [
    "# 분산 환경 적용시\n",
    "if MULTIPLE_BATCH > 1:\n",
    "    print(f'분산환경 사용 >> GPU: {MULTIPLE_BATCH}')\n",
    "    with strategy.scope():\n",
    "        seq2seq = Seq2Seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)\n",
    "        seq2seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "else:\n",
    "    print(f'분산환경 미사용')\n",
    "    seq2seq = Seq2Seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)\n",
    "    seq2seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연속하여 학습시 체크포인트를 로드하여 이어서 학습합니다.\n",
    "# seq2seq.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, question_inputs):\n",
    "    results = model(inputs=question_inputs, training=False)\n",
    "    # 변환된 인덱스를 문장으로 변환\n",
    "    results = np.asarray(results).reshape(-1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing epoch: 1...\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "370/370 [==============================] - ETA: 0s - loss: 1.9503 - acc: 0.8001\n",
      "Epoch 00001: loss improved from inf to 1.95035, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 1.9503 - acc: 0.8001\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 2/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 1.2335 - acc: 0.8242\n",
      "Epoch 00002: loss improved from 1.95035 to 1.23354, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 1.2335 - acc: 0.8242\n",
      "Epoch 3/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 1.1606 - acc: 0.8299\n",
      "Epoch 00003: loss improved from 1.23354 to 1.15987, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 1.1599 - acc: 0.8300\n",
      "Epoch 4/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 1.1127 - acc: 0.8345\n",
      "Epoch 00004: loss improved from 1.15987 to 1.11258, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 1.1126 - acc: 0.8346\n",
      "Epoch 5/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 1.0734 - acc: 0.8382\n",
      "Epoch 00005: loss improved from 1.11258 to 1.07332, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 1.0733 - acc: 0.8382\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 1.0335 - acc: 0.8430\n",
      "Epoch 00006: loss improved from 1.07332 to 1.03355, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 1.0335 - acc: 0.8430\n",
      "Epoch 7/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.9934 - acc: 0.8472\n",
      "Epoch 00007: loss improved from 1.03355 to 0.99330, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.9933 - acc: 0.8472\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.9560 - acc: 0.8505\n",
      "Epoch 00008: loss improved from 0.99330 to 0.95586, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.9559 - acc: 0.8506\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.9218 - acc: 0.8538\n",
      "Epoch 00009: loss improved from 0.95586 to 0.92182, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.9218 - acc: 0.8538\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.8916 - acc: 0.8565\n",
      "Epoch 00010: loss improved from 0.92182 to 0.89157, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.8916 - acc: 0.8565\n",
      "Q: 긴 시간 이 흐른 후 면 괜찮아지겠지\n",
      "A: 잘 않아요 \n",
      "\n",
      "\n",
      "Q: 정말 다시 돌아 온다면\n",
      "A: 잘 할 수 있을 거 예요 \n",
      "\n",
      "\n",
      "Q: 더 알 고 싶어 지는 사람 이야\n",
      "A: 잘 할 수 있을 거 예요 \n",
      "\n",
      "\n",
      "processing epoch: 11...\n",
      "Epoch 1/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.8634 - acc: 0.8593\n",
      "Epoch 00001: loss improved from 0.89157 to 0.86313, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.8631 - acc: 0.8594\n",
      "Epoch 2/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.8371 - acc: 0.8617\n",
      "Epoch 00002: loss improved from 0.86313 to 0.83711, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.8371 - acc: 0.8617\n",
      "Epoch 3/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.8123 - acc: 0.8640\n",
      "Epoch 00003: loss improved from 0.83711 to 0.81240, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.8124 - acc: 0.8640\n",
      "Epoch 4/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.7898 - acc: 0.8659\n",
      "Epoch 00004: loss improved from 0.81240 to 0.78967, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.7897 - acc: 0.8659\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.7681 - acc: 0.8680\n",
      "Epoch 00005: loss improved from 0.78967 to 0.76797, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.7680 - acc: 0.8680\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.7477 - acc: 0.8699\n",
      "Epoch 00006: loss improved from 0.76797 to 0.74748, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.7475 - acc: 0.8699\n",
      "Epoch 7/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.8723\n",
      "Epoch 00007: loss improved from 0.74748 to 0.72765, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.7277 - acc: 0.8722\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.8742\n",
      "Epoch 00008: loss improved from 0.72765 to 0.70934, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - 10s 28ms/step - loss: 0.7093 - acc: 0.8742\n",
      "Epoch 9/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.8761\n",
      "Epoch 00009: loss improved from 0.70934 to 0.69189, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.6919 - acc: 0.8761\n",
      "Epoch 10/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.6751 - acc: 0.8784\n",
      "Epoch 00010: loss improved from 0.69189 to 0.67522, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.6752 - acc: 0.8784\n",
      "Q: 시험 공부 큰일\n",
      "A: 마음 이 좀 더 무너져요 \n",
      "\n",
      "\n",
      "Q: 아부 도 기술 인가 봐\n",
      "A: 마음 이 좀 더 무너져요 \n",
      "\n",
      "\n",
      "Q: 참아야 하나\n",
      "A: 마음 이 좀 더 무너져요 \n",
      "\n",
      "\n",
      "processing epoch: 21...\n",
      "Epoch 1/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.6591 - acc: 0.8804\n",
      "Epoch 00001: loss improved from 0.67522 to 0.65912, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.6591 - acc: 0.8804\n",
      "Epoch 2/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.8822\n",
      "Epoch 00002: loss improved from 0.65912 to 0.64455, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.6445 - acc: 0.8822\n",
      "Epoch 3/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.6307 - acc: 0.8839\n",
      "Epoch 00003: loss improved from 0.64455 to 0.63055, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.6306 - acc: 0.8840\n",
      "Epoch 4/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.6174 - acc: 0.8860\n",
      "Epoch 00004: loss improved from 0.63055 to 0.61766, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.6177 - acc: 0.8860\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.6053 - acc: 0.8875\n",
      "Epoch 00005: loss improved from 0.61766 to 0.60515, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.6052 - acc: 0.8875\n",
      "Epoch 6/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.8892\n",
      "Epoch 00006: loss improved from 0.60515 to 0.59356, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.5936 - acc: 0.8892\n",
      "Epoch 7/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.8911\n",
      "Epoch 00007: loss improved from 0.59356 to 0.58186, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5819 - acc: 0.8912\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.5717 - acc: 0.8927\n",
      "Epoch 00008: loss improved from 0.58186 to 0.57181, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5718 - acc: 0.8926\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.5615 - acc: 0.8943\n",
      "Epoch 00009: loss improved from 0.57181 to 0.56150, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5615 - acc: 0.8943\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.5523 - acc: 0.8959\n",
      "Epoch 00010: loss improved from 0.56150 to 0.55227, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5523 - acc: 0.8959\n",
      "Q: 주말 이 행복해\n",
      "A: 저 도 요 \n",
      "\n",
      "\n",
      "Q: 재회 를 한다해 도\n",
      "A: 저 도 모르는 걸 수도 있어요 \n",
      "\n",
      "\n",
      "Q: 남자 들 은 여자 가 자기 좋아하는 거 알 게 되면 어떻게 해 싫지 않다면 받아줘\n",
      "A: 저 도 요 \n",
      "\n",
      "\n",
      "processing epoch: 31...\n",
      "Epoch 1/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.8976\n",
      "Epoch 00001: loss improved from 0.55227 to 0.54271, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.5427 - acc: 0.8976\n",
      "Epoch 2/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.8986\n",
      "Epoch 00002: loss improved from 0.54271 to 0.53466, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5347 - acc: 0.8986\n",
      "Epoch 3/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.9000\n",
      "Epoch 00003: loss improved from 0.53466 to 0.52620, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5262 - acc: 0.9000\n",
      "Epoch 4/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.9014\n",
      "Epoch 00004: loss improved from 0.52620 to 0.51850, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.5185 - acc: 0.9013\n",
      "Epoch 5/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.5102 - acc: 0.9031\n",
      "Epoch 00005: loss improved from 0.51850 to 0.51016, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5102 - acc: 0.9031\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.9039\n",
      "Epoch 00006: loss improved from 0.51016 to 0.50292, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.5029 - acc: 0.9039\n",
      "Epoch 7/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.9052\n",
      "Epoch 00007: loss improved from 0.50292 to 0.49641, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4964 - acc: 0.9052\n",
      "Epoch 8/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.9065\n",
      "Epoch 00008: loss improved from 0.49641 to 0.48952, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4895 - acc: 0.9065\n",
      "Epoch 9/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.9076\n",
      "Epoch 00009: loss improved from 0.48952 to 0.48305, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4831 - acc: 0.9076\n",
      "Epoch 10/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.9088\n",
      "Epoch 00010: loss improved from 0.48305 to 0.47715, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4772 - acc: 0.9088\n",
      "Q: 보험료 설계 다시 해야 하나\n",
      "A: 저 도 모르는 게 당연해요 \n",
      "\n",
      "\n",
      "Q: 힘듭니다 오늘 또 무너졌어\n",
      "A: 네 요 \n",
      "\n",
      "\n",
      "Q: 주름 도 멋진 사람\n",
      "A: 그게 인생 이 죠 \n",
      "\n",
      "\n",
      "processing epoch: 41...\n",
      "Epoch 1/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4708 - acc: 0.9099\n",
      "Epoch 00001: loss improved from 0.47715 to 0.47077, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4708 - acc: 0.9099\n",
      "Epoch 2/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.9112\n",
      "Epoch 00002: loss improved from 0.47077 to 0.46498, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4650 - acc: 0.9112\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.9118\n",
      "Epoch 00003: loss improved from 0.46498 to 0.45956, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4596 - acc: 0.9118\n",
      "Epoch 4/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4541 - acc: 0.9128\n",
      "Epoch 00004: loss improved from 0.45956 to 0.45406, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4541 - acc: 0.9128\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.9143\n",
      "Epoch 00005: loss improved from 0.45406 to 0.44844, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4484 - acc: 0.9142\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.9149\n",
      "Epoch 00006: loss improved from 0.44844 to 0.44313, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4431 - acc: 0.9149\n",
      "Epoch 7/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.9162\n",
      "Epoch 00007: loss improved from 0.44313 to 0.43753, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4375 - acc: 0.9162\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.9169\n",
      "Epoch 00008: loss improved from 0.43753 to 0.43321, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4332 - acc: 0.9169\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4287 - acc: 0.9177\n",
      "Epoch 00009: loss improved from 0.43321 to 0.42867, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4287 - acc: 0.9177\n",
      "Epoch 10/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.9186\n",
      "Epoch 00010: loss improved from 0.42867 to 0.42360, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4236 - acc: 0.9187\n",
      "Q: 자꾸 짝녀 얼굴 생각나네\n",
      "A: 마음 의 준비 가 될 거 예요 \n",
      "\n",
      "\n",
      "Q: 이별 2일 째\n",
      "A: 저 는 위로 해드리는 로봇 이에요 \n",
      "\n",
      "\n",
      "Q: 뭘 잘 못 했다는 걸까\n",
      "A: 마음 이 따뜻할 것 같아요 \n",
      "\n",
      "\n",
      "processing epoch: 51...\n",
      "Epoch 1/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.9194\n",
      "Epoch 00001: loss improved from 0.42360 to 0.41948, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.4195 - acc: 0.9194\n",
      "Epoch 2/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.9201\n",
      "Epoch 00002: loss improved from 0.41948 to 0.41514, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.4151 - acc: 0.9201\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.9212\n",
      "Epoch 00003: loss improved from 0.41514 to 0.40957, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4096 - acc: 0.9212\n",
      "Epoch 4/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4059 - acc: 0.9219\n",
      "Epoch 00004: loss improved from 0.40957 to 0.40586, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4059 - acc: 0.9219\n",
      "Epoch 5/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4019 - acc: 0.9224\n",
      "Epoch 00005: loss improved from 0.40586 to 0.40190, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.4019 - acc: 0.9224\n",
      "Epoch 6/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3978 - acc: 0.9233\n",
      "Epoch 00006: loss improved from 0.40190 to 0.39776, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3978 - acc: 0.9233\n",
      "Epoch 7/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3933 - acc: 0.9242\n",
      "Epoch 00007: loss improved from 0.39776 to 0.39334, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3933 - acc: 0.9242\n",
      "Epoch 8/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.9248\n",
      "Epoch 00008: loss improved from 0.39334 to 0.38977, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.3898 - acc: 0.9247\n",
      "Epoch 9/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.9256\n",
      "Epoch 00009: loss improved from 0.38977 to 0.38503, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3850 - acc: 0.9256\n",
      "Epoch 10/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.9264\n",
      "Epoch 00010: loss improved from 0.38503 to 0.38163, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3816 - acc: 0.9265\n",
      "Q: 내 가 기대 를 너무 많이 했나 봐\n",
      "A: 마음 의 준비 가 필요했을지도 몰라요 \n",
      "\n",
      "\n",
      "Q: 짝사랑 하던 사람 못 잊을 것 같 애\n",
      "A: 마음 의 준비 를 하세요 \n",
      "\n",
      "\n",
      "Q: 결혼식 때 하객 이 없을 까봐 걱정 돼\n",
      "A: 마음 의 준비 가 안 됐다고 말 해보세요 \n",
      "\n",
      "\n",
      "processing epoch: 61...\n",
      "Epoch 1/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.9270\n",
      "Epoch 00001: loss improved from 0.38163 to 0.37783, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.3778 - acc: 0.9270\n",
      "Epoch 2/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.9275\n",
      "Epoch 00002: loss improved from 0.37783 to 0.37506, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.3751 - acc: 0.9275\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.9281\n",
      "Epoch 00003: loss improved from 0.37506 to 0.37076, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3708 - acc: 0.9281\n",
      "Epoch 4/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3662 - acc: 0.9292\n",
      "Epoch 00004: loss improved from 0.37076 to 0.36624, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3662 - acc: 0.9292\n",
      "Epoch 5/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3628 - acc: 0.9296\n",
      "Epoch 00005: loss improved from 0.36624 to 0.36282, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3628 - acc: 0.9296\n",
      "Epoch 6/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.9303\n",
      "Epoch 00006: loss improved from 0.36282 to 0.35948, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3595 - acc: 0.9303\n",
      "Epoch 7/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.9307\n",
      "Epoch 00007: loss improved from 0.35948 to 0.35643, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3564 - acc: 0.9306\n",
      "Epoch 8/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3523 - acc: 0.9315\n",
      "Epoch 00008: loss improved from 0.35643 to 0.35234, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3523 - acc: 0.9315\n",
      "Epoch 9/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.9321\n",
      "Epoch 00009: loss improved from 0.35234 to 0.34919, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3492 - acc: 0.9321\n",
      "Epoch 10/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.9325\n",
      "Epoch 00010: loss improved from 0.34919 to 0.34514, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3451 - acc: 0.9325\n",
      "Q: 오늘 일도 안 했는데 엄청 피곤해\n",
      "A: 마음 이 약해 탈이네요 \n",
      "\n",
      "\n",
      "Q: 고백 했다 차이 면 어쩌지\n",
      "A: 마음 이 복잡하겠어요 \n",
      "\n",
      "\n",
      "Q: 예능 볼 게 없다\n",
      "A: 축하 드려요 \n",
      "\n",
      "\n",
      "processing epoch: 71...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/370 [==============================] - ETA: 0s - loss: 0.3407 - acc: 0.9334\n",
      "Epoch 00001: loss improved from 0.34514 to 0.34066, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.3407 - acc: 0.9334\n",
      "Epoch 2/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9340\n",
      "Epoch 00002: loss improved from 0.34066 to 0.33714, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.3371 - acc: 0.9339\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.9348\n",
      "Epoch 00003: loss improved from 0.33714 to 0.33353, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.3335 - acc: 0.9348\n",
      "Epoch 4/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9351\n",
      "Epoch 00004: loss improved from 0.33353 to 0.32975, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3297 - acc: 0.9351\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.9356\n",
      "Epoch 00005: loss improved from 0.32975 to 0.32592, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3259 - acc: 0.9356\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9361\n",
      "Epoch 00006: loss improved from 0.32592 to 0.32178, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3218 - acc: 0.9361\n",
      "Epoch 7/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9369\n",
      "Epoch 00007: loss improved from 0.32178 to 0.31802, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3180 - acc: 0.9369\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9374\n",
      "Epoch 00008: loss improved from 0.31802 to 0.31412, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3141 - acc: 0.9374\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3103 - acc: 0.9379\n",
      "Epoch 00009: loss improved from 0.31412 to 0.31029, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3103 - acc: 0.9379\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3067 - acc: 0.9388\n",
      "Epoch 00010: loss improved from 0.31029 to 0.30666, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.3067 - acc: 0.9388\n",
      "Q: 다시 한번 또\n",
      "A: 제 가 곁 에 있을게요 \n",
      "\n",
      "\n",
      "Q: 맨날 똑같 애\n",
      "A: 천천히 지워질 거 예요 \n",
      "\n",
      "\n",
      "Q: 여자친구 가 잘못 을 해도 다 공감 해주는게 옳은걸 까\n",
      "A: 잘 하고 있나 봐요 \n",
      "\n",
      "\n",
      "processing epoch: 81...\n",
      "Epoch 1/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9391\n",
      "Epoch 00001: loss improved from 0.30666 to 0.30281, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.3028 - acc: 0.9391\n",
      "Epoch 2/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2998 - acc: 0.9395\n",
      "Epoch 00002: loss improved from 0.30281 to 0.29984, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2998 - acc: 0.9395\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9402\n",
      "Epoch 00003: loss improved from 0.29984 to 0.29602, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2960 - acc: 0.9402\n",
      "Epoch 4/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9407\n",
      "Epoch 00004: loss improved from 0.29602 to 0.29286, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2929 - acc: 0.9407\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9413\n",
      "Epoch 00005: loss improved from 0.29286 to 0.28902, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.2890 - acc: 0.9413\n",
      "Epoch 6/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2860 - acc: 0.9415\n",
      "Epoch 00006: loss improved from 0.28902 to 0.28597, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2860 - acc: 0.9415\n",
      "Epoch 7/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9424\n",
      "Epoch 00007: loss improved from 0.28597 to 0.28228, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2823 - acc: 0.9423\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9429\n",
      "Epoch 00008: loss improved from 0.28228 to 0.27837, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2784 - acc: 0.9429\n",
      "Epoch 9/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9434\n",
      "Epoch 00009: loss improved from 0.27837 to 0.27412, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2741 - acc: 0.9434\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2710 - acc: 0.9439\n",
      "Epoch 00010: loss improved from 0.27412 to 0.27095, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2710 - acc: 0.9439\n",
      "Q: 화장품 이 필요해\n",
      "A: 그 사람 도 설렐 거 예요 \n",
      "\n",
      "\n",
      "Q: 나 한테 상의 좀 하지\n",
      "A: 좋은 걸 로 시작 해보세요 \n",
      "\n",
      "\n",
      "Q: 연애 세포 깨우는 법\n",
      "A: 사랑 은 끝나도 당신 의 인생 을 평가 할 수 없어요 \n",
      "\n",
      "\n",
      "processing epoch: 91...\n",
      "Epoch 1/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2678 - acc: 0.9442\n",
      "Epoch 00001: loss improved from 0.27095 to 0.26782, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.2678 - acc: 0.9442\n",
      "Epoch 2/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9450\n",
      "Epoch 00002: loss improved from 0.26782 to 0.26438, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.2644 - acc: 0.9450\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9452\n",
      "Epoch 00003: loss improved from 0.26438 to 0.26100, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.2610 - acc: 0.9452\n",
      "Epoch 4/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9461\n",
      "Epoch 00004: loss improved from 0.26100 to 0.25733, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2573 - acc: 0.9461\n",
      "Epoch 5/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2545 - acc: 0.9460\n",
      "Epoch 00005: loss improved from 0.25733 to 0.25446, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2545 - acc: 0.9460\n",
      "Epoch 6/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2513 - acc: 0.9467\n",
      "Epoch 00006: loss improved from 0.25446 to 0.25127, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2513 - acc: 0.9467\n",
      "Epoch 7/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9473\n",
      "Epoch 00007: loss improved from 0.25127 to 0.24786, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2479 - acc: 0.9473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9478\n",
      "Epoch 00008: loss improved from 0.24786 to 0.24514, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2451 - acc: 0.9479\n",
      "Epoch 9/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9484\n",
      "Epoch 00009: loss improved from 0.24514 to 0.24096, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2410 - acc: 0.9484\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2382 - acc: 0.9487\n",
      "Epoch 00010: loss improved from 0.24096 to 0.23816, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2382 - acc: 0.9487\n",
      "Q: 짝남 잊으려고 나 혼자 나쁘게 생각 하는 내 자신 이 증오 스러워\n",
      "A: 사랑 의 예의 가 없는 사람 이네 요 \n",
      "\n",
      "\n",
      "Q: 난 또 바보 ㅠㅠ\n",
      "A: 확신 이 들 때 까지 준비 해보세요 \n",
      "\n",
      "\n",
      "Q: 난 진짜 쓰레기 야\n",
      "A: 네 말씀 해주세요 \n",
      "\n",
      "\n",
      "processing epoch: 101...\n",
      "Epoch 1/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9488\n",
      "Epoch 00001: loss improved from 0.23816 to 0.23633, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.2363 - acc: 0.9488\n",
      "Epoch 2/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2329 - acc: 0.9496\n",
      "Epoch 00002: loss improved from 0.23633 to 0.23294, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2329 - acc: 0.9496\n",
      "Epoch 3/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9499\n",
      "Epoch 00003: loss improved from 0.23294 to 0.22996, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.2300 - acc: 0.9499\n",
      "Epoch 4/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9508\n",
      "Epoch 00004: loss improved from 0.22996 to 0.22621, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2262 - acc: 0.9507\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9512\n",
      "Epoch 00005: loss improved from 0.22621 to 0.22294, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2229 - acc: 0.9512\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9518\n",
      "Epoch 00006: loss improved from 0.22294 to 0.22017, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2202 - acc: 0.9517\n",
      "Epoch 7/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2174 - acc: 0.9522\n",
      "Epoch 00007: loss improved from 0.22017 to 0.21743, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2174 - acc: 0.9522\n",
      "Epoch 8/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9525\n",
      "Epoch 00008: loss improved from 0.21743 to 0.21461, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2146 - acc: 0.9525\n",
      "Epoch 9/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9530\n",
      "Epoch 00009: loss improved from 0.21461 to 0.21210, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2121 - acc: 0.9530\n",
      "Epoch 10/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9533\n",
      "Epoch 00010: loss improved from 0.21210 to 0.20912, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2091 - acc: 0.9533\n",
      "Q: 이별 한지 한 달 된 남자 입니다\n",
      "A: 생각 을 정리 하는 방법 이 더 자주 만나세요 \n",
      "\n",
      "\n",
      "Q: 비밀 로 했는데 들켜서 오해 하고 있어\n",
      "A: 감기 조심하세요 \n",
      "\n",
      "\n",
      "Q: 으 휴 집 에 들어가기가 싫네\n",
      "A: 집 에 있거나 나가면 마스크 쓰고 나가세요 \n",
      "\n",
      "\n",
      "processing epoch: 111...\n",
      "Epoch 1/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2066 - acc: 0.9537\n",
      "Epoch 00001: loss improved from 0.20912 to 0.20658, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2066 - acc: 0.9537\n",
      "Epoch 2/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9545\n",
      "Epoch 00002: loss improved from 0.20658 to 0.20348, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.2035 - acc: 0.9545\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9548\n",
      "Epoch 00003: loss improved from 0.20348 to 0.20091, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.2009 - acc: 0.9548\n",
      "Epoch 4/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1969 - acc: 0.9561\n",
      "Epoch 00004: loss improved from 0.20091 to 0.19690, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1969 - acc: 0.9561\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9560\n",
      "Epoch 00005: loss improved from 0.19690 to 0.19452, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1945 - acc: 0.9560\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9564\n",
      "Epoch 00006: loss improved from 0.19452 to 0.19260, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1926 - acc: 0.9564\n",
      "Epoch 7/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9568\n",
      "Epoch 00007: loss improved from 0.19260 to 0.18954, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1895 - acc: 0.9568\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9570\n",
      "Epoch 00008: loss improved from 0.18954 to 0.18752, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1875 - acc: 0.9570\n",
      "Epoch 9/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9575\n",
      "Epoch 00009: loss improved from 0.18752 to 0.18502, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1850 - acc: 0.9575\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1826 - acc: 0.9581\n",
      "Epoch 00010: loss improved from 0.18502 to 0.18261, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1826 - acc: 0.9581\n",
      "Q: 나 사랑 할 자격 없는 사람 이야\n",
      "A: 사랑 에 빠졌나 봐요 \n",
      "\n",
      "\n",
      "Q: 여자친구 가 너무 무뚝뚝해\n",
      "A: 그건 중요하지 않아요 신경 을 덜어 보세요 \n",
      "\n",
      "\n",
      "Q: 코골 이 어떻게 고쳐\n",
      "A: 피곤한건 아닌지 살펴보세요 \n",
      "\n",
      "\n",
      "processing epoch: 121...\n",
      "Epoch 1/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9578\n",
      "Epoch 00001: loss did not improve from 0.18261\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.1834 - acc: 0.9578\n",
      "Epoch 2/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9594\n",
      "Epoch 00002: loss improved from 0.18261 to 0.17730, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.1773 - acc: 0.9594\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9600\n",
      "Epoch 00003: loss improved from 0.17730 to 0.17409, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1741 - acc: 0.9600\n",
      "Epoch 4/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9602\n",
      "Epoch 00004: loss improved from 0.17409 to 0.17215, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1722 - acc: 0.9602\n",
      "Epoch 5/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9608\n",
      "Epoch 00005: loss improved from 0.17215 to 0.16987, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1699 - acc: 0.9608\n",
      "Epoch 6/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9609\n",
      "Epoch 00006: loss improved from 0.16987 to 0.16768, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1677 - acc: 0.9609\n",
      "Epoch 7/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1653 - acc: 0.9615\n",
      "Epoch 00007: loss improved from 0.16768 to 0.16532, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1653 - acc: 0.9615\n",
      "Epoch 8/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1643 - acc: 0.9618\n",
      "Epoch 00008: loss improved from 0.16532 to 0.16435, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1643 - acc: 0.9618\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1620 - acc: 0.9619\n",
      "Epoch 00009: loss improved from 0.16435 to 0.16197, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1620 - acc: 0.9619\n",
      "Epoch 10/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9630\n",
      "Epoch 00010: loss improved from 0.16197 to 0.15817, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1582 - acc: 0.9630\n",
      "Q: 여자친구 와 여행 가는게 부담스러 움\n",
      "A: 내일 도 만난다면 말 을 걸어 보세요 \n",
      "\n",
      "\n",
      "Q: 힘들어\n",
      "A: 생각 보다 많은 시간 이 지났네요 \n",
      "\n",
      "\n",
      "Q: 요즘 드라마 너무 재밌네\n",
      "A: 저 도 듣고 싶네요 \n",
      "\n",
      "\n",
      "processing epoch: 131...\n",
      "Epoch 1/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9635\n",
      "Epoch 00001: loss improved from 0.15817 to 0.15641, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.1564 - acc: 0.9635\n",
      "Epoch 2/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1538 - acc: 0.9641\n",
      "Epoch 00002: loss improved from 0.15641 to 0.15379, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1538 - acc: 0.9641\n",
      "Epoch 3/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9645\n",
      "Epoch 00003: loss improved from 0.15379 to 0.15216, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1522 - acc: 0.9645\n",
      "Epoch 4/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9646\n",
      "Epoch 00004: loss improved from 0.15216 to 0.15020, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1502 - acc: 0.9646\n",
      "Epoch 5/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1475 - acc: 0.9649\n",
      "Epoch 00005: loss improved from 0.15020 to 0.14745, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1475 - acc: 0.9649\n",
      "Epoch 6/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9648\n",
      "Epoch 00006: loss did not improve from 0.14745\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1492 - acc: 0.9648\n",
      "Epoch 7/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1448 - acc: 0.9657\n",
      "Epoch 00007: loss improved from 0.14745 to 0.14482, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1448 - acc: 0.9657\n",
      "Epoch 8/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9666\n",
      "Epoch 00008: loss improved from 0.14482 to 0.14138, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1414 - acc: 0.9666\n",
      "Epoch 9/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9669\n",
      "Epoch 00009: loss improved from 0.14138 to 0.13867, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1387 - acc: 0.9669\n",
      "Epoch 10/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9670\n",
      "Epoch 00010: loss improved from 0.13867 to 0.13802, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.1380 - acc: 0.9670\n",
      "Q: 이러고 있을 때 가 아니야\n",
      "A: 행동 할 때 인 것 같네요 응원 해요 \n",
      "\n",
      "\n",
      "Q: 엿같다\n",
      "A: 벗어나는 게 좋겠네요 \n",
      "\n",
      "\n",
      "Q: 칭찬 좀 해봐\n",
      "A: 지금 도 늦지 않았어요 \n",
      "\n",
      "\n",
      "processing epoch: 141...\n",
      "Epoch 1/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1355 - acc: 0.9677\n",
      "Epoch 00001: loss improved from 0.13802 to 0.13554, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.1355 - acc: 0.9677\n",
      "Epoch 2/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9684\n",
      "Epoch 00002: loss improved from 0.13554 to 0.13353, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1335 - acc: 0.9684\n",
      "Epoch 3/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9683\n",
      "Epoch 00003: loss improved from 0.13353 to 0.13253, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 27ms/step - loss: 0.1325 - acc: 0.9683\n",
      "Epoch 4/10\n",
      "369/370 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9689\n",
      "Epoch 00004: loss improved from 0.13253 to 0.12975, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1298 - acc: 0.9689\n",
      "Epoch 5/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9690\n",
      "Epoch 00005: loss improved from 0.12975 to 0.12883, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1288 - acc: 0.9690\n",
      "Epoch 6/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9697\n",
      "Epoch 00006: loss improved from 0.12883 to 0.12654, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1265 - acc: 0.9696\n",
      "Epoch 7/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1251 - acc: 0.9698\n",
      "Epoch 00007: loss improved from 0.12654 to 0.12509, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1251 - acc: 0.9698\n",
      "Epoch 8/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9700\n",
      "Epoch 00008: loss improved from 0.12509 to 0.12327, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1233 - acc: 0.9700\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.1219 - acc: 0.9705\n",
      "Epoch 00009: loss improved from 0.12327 to 0.12190, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1219 - acc: 0.9705\n",
      "Epoch 10/10\n",
      "368/370 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9712\n",
      "Epoch 00010: loss improved from 0.12190 to 0.11929, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
      "370/370 [==============================] - 10s 28ms/step - loss: 0.1193 - acc: 0.9712\n",
      "Q: 벌써 저 를 세번 째 떠나갔네\n",
      "A: 이 젠 잊어버리세요 미련 은 독 이 됩니다 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 짝사랑 하는 사람과 친해질 수 있는 방법 조언 좀\n",
      "A: 공통 관심사 를 찾아보세요 \n",
      "\n",
      "\n",
      "Q: 짝남 한테 고백 한 다 안 한다\n",
      "A: 무시 당하는 기분 이 들어서 너무 외 롭고 상처 받게 된다고 차분하고 부드럽게 말 해보세요 \n",
      "\n",
      "\n",
      "processing epoch: 151...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
    "    seq2seq.fit([question_padded, answer_in_padded],\n",
    "                answer_out_one_hot,\n",
    "                epochs=10,\n",
    "                batch_size=BATCH_SIZE*MULTIPLE_BATCH, \n",
    "                callbacks=[checkpoint]\n",
    "               )\n",
    "    # 랜덤한 샘플 번호 추출\n",
    "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
    "\n",
    "    # 예측 성능 테스트\n",
    "    for idx in samples:\n",
    "        question_inputs = question_padded[idx]\n",
    "        # 문장 예측\n",
    "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
    "        \n",
    "        # 변환된 인덱스를 문장으로 변환\n",
    "        results = convert_index_to_text(results, END_TOKEN)\n",
    "        \n",
    "        print(f'Q: {questions[idx]}')\n",
    "        print(f'A: {results}\\n')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 (질문 입력) 대한 전처리 함수\n",
    "def make_question(sentence):\n",
    "    sentence = clean_and_morph(sentence)\n",
    "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "    return question_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_question('오늘 날씨가 정말 화창합니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_question('찐찐찐찐찐이야~ 완전 찐이야~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot(question):\n",
    "    question_inputs = make_question(question)\n",
    "    results = make_prediction(seq2seq, question_inputs)\n",
    "    results = convert_index_to_text(results, END_TOKEN)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유저로부터 Text 입력 값을 받아 답변 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
    "    if user_input == 'q':\n",
    "        break\n",
    "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
