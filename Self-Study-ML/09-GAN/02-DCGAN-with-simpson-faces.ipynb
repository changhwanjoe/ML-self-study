{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings \n",
    "import time\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROPPED_DIR = 'data/cropped/'\n",
    "SIMPLIFIED_DIR = 'data/simplified/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.initializers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z=(100,)):\n",
    "    input_ = Input(z)\n",
    "    \n",
    "    # 4 X 4, 512\n",
    "    x = Dense(4*4*512, activation=LeakyReLU(0.2))(input_)\n",
    "    x = Reshape((4, 4, 512))(x)\n",
    "    \n",
    "    # 8 X 8, 512\n",
    "    x = Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', kernel_initializer=TruncatedNormal(stddev=0.02))(x) # strides=(2, 2) 사진의 크기를 2배로 키움\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # 16 X 16, 256\n",
    "    x = Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', kernel_initializer=TruncatedNormal(stddev=0.02))(x) # strides=(2, 2) 사진의 크기를 2배로 키움\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # 32 X 32, 128\n",
    "    x = Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer=TruncatedNormal(stddev=0.02))(x) # strides=(2, 2) 사진의 크기를 2배로 키움\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # 64 X 64, 64\n",
    "    x = Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', kernel_initializer=TruncatedNormal(stddev=0.02))(x) # strides=(2, 2) 사진의 크기를 2배로 키움\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # 64 X 64, 3\n",
    "    x = Conv2DTranspose(3, (5, 5), padding='same', kernel_initializer=TruncatedNormal(stddev=0.02), activation='tanh')(x)\n",
    "    model = Model(input_, x)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(input_shape=(64, 64, 3)):\n",
    "    input_layer= Input(input_shape)\n",
    "    \n",
    "    x = Conv2D(64, (5, 5), padding='same', strides=(2, 2), kernel_initializer=TruncatedNormal(stddev=0.02))(input_layer)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(128, (5, 5), padding='same', strides=(2, 2), kernel_initializer=TruncatedNormal(stddev=0.02))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(256, (5, 5), padding='same', strides=(2, 2), kernel_initializer=TruncatedNormal(stddev=0.02))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(512, (5, 5), padding='same', strides=(2, 2), kernel_initializer=TruncatedNormal(stddev=0.02))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=0.00005)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(input_layer, x)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', metrics=['acc'], optimizer=Adam(lr=0.00004, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gan 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_noise = Input(shape=(100, ))\n",
    "x = generator(input_noise)\n",
    "gan_output = discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = Model(input_noise, gan_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0004, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outlier 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_image = [\"9746\",\"9731\",\"9717\",\"9684\",\"9637\",\"9641\",\"9642\",\"9584\",\"9541\",\"9535\",\n",
    "\"9250\",\"9251\",\"9252\",\"9043\",\"8593\",\"8584\",\"8052\",\"8051\",\"8008\",\"7957\",\n",
    "\"7958\"\"7761\",\"7762\",\"9510\",\"9307\",\"4848\",\"4791\",\"4785\",\"4465\",\"2709\",\n",
    "\"7724\",\"7715\",\"7309\",\"7064\",\"7011\",\"6961\",\"6962\",\"6963\",\"6960\",\"6949\",\n",
    "\"6662\",\"6496\",\"6409\",\"6411\",\"6406\",\"6407\",\"6170\",\"6171\",\"6172\",\"5617\",\n",
    "\"4363\",\"4232\",\"4086\",\"4047\",\"3894\",\"3889\",\"3493\",\"3393\",\"3362\",\"2780\",\n",
    "\"2710\",\"2707\",\"2708\",\"2711\",\"2712\",\"2309\",\"2056\",\"1943\",\"1760\",\"1743\",\n",
    "\"1702\",\"1281\",\"1272\",\"772\",\"736\",\"737\",\"691\",\"684\",\"314\",\"242\",\"191\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(CROPPED_DIR + '9717.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_image = [x + '.png' for x in exclude_image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = np.asarray([np.asarray(Image.open(x).resize((64, 64))) for x in glob(CROPPED_DIR + '*') if x not in exclude_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "np.random.shuffle(input_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 생성\n",
    "def get_batches(data):\n",
    "    batches = []\n",
    "    batch_size = 64\n",
    "    for i in range(int(data.shape[0] // batch_size)):\n",
    "        batch = data[i * batch_size: (i + 1) * batch_size]\n",
    "        augmented_images = []\n",
    "        for image in batch:\n",
    "            image = Image.fromarray(image)\n",
    "            # 확률적으로 Image 횡방향 flip\n",
    "            if random.choice([True, False]):\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            augmented_images.append(np.asarray(image))\n",
    "        batch = np.asarray(augmented_images)\n",
    "        normalized_batch = (batch / 127.5)-1\n",
    "        batches.append(normalized_batch)\n",
    "        \n",
    "    return np.asarray(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(sample_images, name, epoch):\n",
    "    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    for index, axis in enumerate(axes):\n",
    "        axis.axis('off')\n",
    "        image_array = sample_images[index]\n",
    "        axis.imshow(image_array)\n",
    "        image = Image.fromarray(image_array)\n",
    "        #image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n",
    "    #plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_epoch(d_losses, g_losses , data_shape, epoch, duration, input_z):\n",
    "    minibatch_size = int(data_shape[0]//BATCH_SIZE)\n",
    "    print(\"Epoch {}/{}\".format(epoch, EPOCHS),\n",
    "          \"\\nDuration: {:.5f}\".format(duration),\n",
    "          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n",
    "          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(d_losses, label='Discriminator', alpha=0.6)\n",
    "    plt.plot(g_losses, label='Generator', alpha=0.6)\n",
    "    plt.title(\"Losses\")\n",
    "    plt.legend()\n",
    "    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    test(input_z, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(input_z, epoch):\n",
    "    samples = generator.predict(input_z[:SAMPLES_TO_SHOW])\n",
    "    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n",
    "    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMAGE_SIZE = 64\n",
    "NOISE_SIZE = 100\n",
    "LR_D = 0.00004\n",
    "LR_G = 0.0004\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50 # For better results increase this value\n",
    "BETA1 = 0.5\n",
    "WEIGHT_INIT_STDDEV = 0.02\n",
    "EPSILON = 0.00005\n",
    "SAMPLES_TO_SHOW = 8\n",
    "OUTPUT_DIR = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_losses = []\n",
    "g_losses = []\n",
    "cum_d_loss = 0\n",
    "cum_g_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    start_time = time.time()\n",
    "    for batch_images in get_batches(input_images):\n",
    "        \n",
    "        noise_data = np.random.normal(0, 1, size=(64, 100))\n",
    "        \n",
    "        generated_images = generator.predict(noise_data)\n",
    "        \n",
    "        noise_prop = 0.05\n",
    "        real_labels = np.zeros((64, 1)) + np.random.uniform(0.0, 0.1, size=(64, 1))\n",
    "        flipped_index = np.random.choice(np.arange(len(real_labels)), size=int(noise_prop * len(real_labels)))\n",
    "        real_labels[flipped_index] = 1 - real_labels[flipped_index]\n",
    "        \n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # Train discriminator on real data\n",
    "        d_loss_real = discriminator.train_on_batch(batch_images, real_labels)\n",
    "\n",
    "        # Prepare labels for generated data\n",
    "        fake_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
    "        flipped_idx = np.random.choice(np.arange(len(fake_labels)), size=int(noise_prop*len(fake_labels)))\n",
    "        fake_labels[flipped_idx] = 1 - fake_labels[flipped_idx]\n",
    "        \n",
    "        # Train discriminator on generated data\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)\n",
    "        \n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        cum_d_loss += d_loss\n",
    "        d_losses.append(d_loss[0])\n",
    "        \n",
    "        \n",
    "        # Train generator\n",
    "        discriminator.trainable = False\n",
    "        noise_data = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_SIZE))\n",
    "        g_loss = gan.train_on_batch(noise_data, np.zeros((BATCH_SIZE, 1)))\n",
    "        cum_g_loss += g_loss\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "    if epoch > 0 and epoch % 20 == 0 :\n",
    "        print(\"saving model\")\n",
    "        discriminator.save_weights(\"desc-simposon-model.h5-\" + str(epoch))\n",
    "        gan.save_weights(\"gan-simposon-model.h5-\" + str(epoch))\n",
    "\n",
    "    # Plot the progress\n",
    "    summarize_epoch(d_losses, g_losses, input_images.shape, epoch, time.time()-start_time, noise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
